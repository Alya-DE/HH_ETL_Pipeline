# My_ETL_Pipeline

Хочу пожелать всем, читающим данную работу, доброго времени суток и поблагодарить за уделенное время!

---
![etl_schema](https://github.com/Alya-DE/My_ETL/blob/main/png/etl_schema.png)

В рамках данного проекта я поставила цель практического освоения процессов ETL. В качестве источника данных был выбран открытый API сайта hh.ru. Задачей было настроить получение данных об опубликованных вакансиях, содержащих ключевые слова "Data Engineer", на начальные позиции в Москве с требованиями к опыту от 0 до 3 лет. В качестве оркестратора был задействован Apache Airflow. Витрины данных было решено хранить в PostgreSQL. Для визуализации был выбран сервис анализа и визуализации данных от Яндекса - DataLens.

---
### Задействованные сервисы:

|       ПО       |           Порт / сайт          |     Версия     |
| -------------- | ------------------------------ | -------------- |
| Apache Airflow |     http://localhost:8080      |    2.10.2      |
| PostgreSQL     |     http://localhost:5432      |      17        |
| DBeaver        |               -                |    24.2.4      |
| DataLens       | https://datalens.yandex.cloud/ |       -        |

---
### Подготовка к работе:

Для выполнения поставленной задачи можно воспользоваться предустановленным программным обеспечением - Apache Airflow и PostgreSQL или развернуть все необходимые инструменты с помощью docker-compose (для реализации данного варианта потребуется также установить ПО Docker и Docker-compose.

---
### Источник данных:
В качестве источника данных был выбран следующий API URL:

https://api.hh.ru/vacancies?text=data%20engineer&area=1&per_page=100&date_from={start_date}&date_to={end_date}&experience=between1And3&experience=noExperience" 

Так как hh.ru позволяет получить исторические данные только за предшествующий месяц, то было принято решение получить данные по опубликованным вакансиям с 1 ноября, чтобы в дальнейшем проводить аналитику данных за полноценный месяц.

---
### Airflow:
С помощью DAG Airflow выполняется создание необходимого отношения в базе данных, в которую будут сохранены извлеченные данные, а также само извлечение необходимых данных с API URL. Для извлечения исторических данных за предшествующий период и для настройки извлечения данных на будущие дни логика была разделена на две функции: load_historical_data_to_db() и load_daily_data_to_db(). Airflow помогает выполнить сразу два этапа ETL - Transfer и Load. В рамках этапа Transfer DAG Airflow преобразует данные из формата JSON из API в вид структурированной таблицы перед загрузкой в базу данных. В рамках этапа Load с помощью все того же DAG осуществляется загрузка данных в базу данных PostgreSQL, где они сохраняются для последующего использования.

---
### PostgreSQL:
В связи с тем, что ожидалось получение небольшого объема данных, было решено выбрать в качестве СУБД PostgreSQL. Была использована 17-я версия PostgreSQL. База данных hh_data_engineer и пользователь worker с паролем password были созданы заранее через интерактивное окружение PostgreSQL. Для PostgreSQL был использован предоставленный порт по умолчанию :5432.

---
### DataLens:
Для визуализации и выполнения аналитики полученных данных был выбран сервис DataLens. На основании полученных данных был построен следующий дашборд:

![visual_analytics](https://github.com/Alya-DE/My_ETL/blob/main/png/visual_analytics.png)
